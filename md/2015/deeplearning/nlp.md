## natural language process

[return to home](../../../README.md)

[attention and memory](#attention and memory)

- A Primer on Neural Network Models for Natural Language Processing. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjr-I3b-9LQAhVEVbwKHVo8A70QFggsMAE&url=http%3A%2F%2Fcs.biu.ac.il%2F~yogo%2Fnnlp.pdf&usg=AFQjCNEZEkggUYseGdLhpFy_iG5mBA3X9g)]
- A Unified Tagging Solution- Bidirectional LSTM Recurrent Neural Network with Word Embedding.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwj-t_LG_NLQAhVES7wKHWTtCmIQFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1511.00215&usg=AFQjCNECqO7dKUb1L7bkvFFR_8-hgPy52w)]
- Alternative structures for character-level RNNs. [[pdf]](docs/2015/Alternative structures for character-level RNNs.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiyx_iZ_dLQAhUBT7wKHUE0A38QFggqMAE&url=http%3A%2F%2Fwww.di.ens.fr%2F~bojanowski%2Fpapers%2Fbojanowski16alternative.pdf&usg=AFQjCNF0ds1vVOijyqtBX-g_s9x8OedIbg)]
- Ask Me Anything- Dynamic Memory Networks for Natural Language Processing. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiH8Kjf_dLQAhXIxLwKHU7QAb8QFggpMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1506.07285&usg=AFQjCNFjdb3GPe1IrNPSh8zevJazf58JwQ)]
- BlackOut- Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies. [[url](https://arxiv.org/abs/1511.06909)]
- <b>Character-Aware Neural Language Models.</b> [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjJ7JjA_9LQAhVDwLwKHbmaCskQFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1508.06615&usg=AFQjCNG3RqYNgZfsn7zIez3SEzwB70cEKg)]
- Character-level Convolutional Networks for Text Classification. [[pdf]](docs/2015/Character-level Convolutional Networks for Text Classification.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiE5qTl_9LQAhUEWLwKHcHaDQgQFggsMAE&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5782-character-level-convolutional-networks-for-text-classification.pdf&usg=AFQjCNFInMceTBvIU8_8XmtKfoizKGpOVA)]
- Deep Speech 2- End-to-End Speech Recognition in English and Mandarin. [[pdf]](docs/2015/Deep Speech 2- End-to-End Speech Recognition in English and Mandarin.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwi5uvCDgdPQAhVFwLwKHSJ6B0MQFggvMAE&url=http%3A%2F%2Fjmlr.org%2Fproceedings%2Fpapers%2Fv48%2Famodei16.pdf&usg=AFQjCNFE5u2Xu81DH2gQUoyHVtC4AzLhYg)]
- <b>Distributed Representations of Sentences and Documents</b>. [[pdf]](docs/2015/Distributed Representations of Sentences and Documents.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjf2ZyrgtPQAhWJvrwKHTV4BqsQFgguMAE&url=http%3A%2F%2Fcs.stanford.edu%2F~quocle%2Fparagraph_vector.pdf&usg=AFQjCNESECVF_9eXAkAjfSqqHrqlxkVQgg)]
- Dynamic Capacity Networks. [[pdf]](docs/2015/Dynamic Capacity Networks.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiN_Yj0gtPQAhVPQLwKHVz0BZwQFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1511.07838&usg=AFQjCNFR0AEK2z-pT8ulDl98QWXP0M3fCw)]
- Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs. [[pdf]](docs/2015/Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjy35b6g9PQAhUDx7wKHULuChcQFggsMAE&url=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FD%2FD15%2FD15-1041.pdf&usg=AFQjCNHhOsMGNAY4FXq5-B9dDjy-OPBWvA)]
- Larger-Context Language Modeling. [[pdf]](docs/2015/Larger-Context Language Modeling.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjHlvWshNPQAhWLybwKHTiuB-0QFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1511.03729&usg=AFQjCNG88NnFeKSPKis1dVnVNYa0Tu50Gw)]
- Multi-task Sequence to Sequence Learning. [[pdf]](docs/2015/Multi-task Sequence to Sequence Learning.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiu7MbphdPQAhWDUrwKHeo3DO8QFggpMAE&url=http%3A%2F%2Fjan.stanford.edu%2Fpubs%2Fluong2016iclr_multi.pdf&usg=AFQjCNGYWAqWnjbi6p3ZXWe0hBcNMSWawA)]
- Natural Language Understanding with Distributed Representation. [[pdf]](docs/2015/Natural Language Understanding with Distributed Representation.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjhm4yThtPQAhWJVrwKHY-fDusQFggiMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1511.07916&usg=AFQjCNEXqHl1eNACFv-e99vq-omz4TcY_Q)]
- Neural Machine Translation of Rare Words with Subword Units. [[arxiv](https://arxiv.org/abs/1508.07909)]
- Neural Responding Machine for Short-Text Conversation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiT-aOMntPQAhXHwbwKHXcqAzYQFggpMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1503.02364&usg=AFQjCNETeT0aqPYSdo-fP-DYyl7WZg2BqQ)]
- Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network. [[pdf]](docs/2015/Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwigxZnPntPQAhUCybwKHRJrCc8QFggiMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1510.06168&usg=AFQjCNEIDEbarhumVAtwYDPin-r5-10mSQ)]
- Reading Scene Text in Deep Convolutional Sequences. [[url](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjnwcaJpY7RAhVnwlQKHdQhBMkQFggrMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1506.04395&usg=AFQjCNFwWEe1FlLYwTvy5JrYCme9M_QREA)]
- Recurrent Convolutional Neural Networks for Text Classification. [[pdf](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwip1u_yqpTRAhXGz1QKHUGDCc8QFggxMAI&url=http%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI15%2Fpaper%2Fdownload%2F9745%2F9552&usg=AFQjCNFn9tI5wMb1f81hDQPHySE9C2OOpA)]
- Semi-supervised Sequence Learning. [[pdf]](docs/2015/Semi-supervised Sequence Learning.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwj28pOnodPQAhUJT7wKHb_eDpIQFggiMAA&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5949-semi-supervised-sequence-learning.pdf&usg=AFQjCNFi_8bOPu361mtaI13MWB_aHDlspg)]
- Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems. [[arxiv](https://arxiv.org/abs/1508.01745)]
- sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&cad=rja&uact=8&ved=0ahUKEwiQ_aHDodPQAhUITLwKHYP2B2kQFgg5MAM&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F36f9%2F886ad1cb9ee3f66c5af0282ae7a3359b86b2.pdf&usg=AFQjCNE7oWW1uaAuK2skjRUnPXhykGFeMw)]
- Sequence Level Training with Recurrent Neural Networks. [[pdf]](docs/2015/Sequence Level Training with Recurrent Neural Networks.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjEgIbnodPQAhUKyrwKHXJICVUQFgghMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1511.06732&usg=AFQjCNGEymvqFAIJnyEUAh0Ok7dImTDJjg)]
- Strategies for Training Large Vocabulary Neural Language Models. [[pdf]](docs/2015/Strategies for Training Large Vocabulary Neural Language Models.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjE1KCTo9PQAhXHwrwKHeTfDZ8QFggsMAE&url=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP%2FP16%2FP16-1186.pdf&usg=AFQjCNGUJ0zFy2G4j9x9enwYuu2iTN5cig)]
- Teaching Machines to Read and Comprehend. [[pdf]](docs/2015/Teaching Machines to Read and Comprehend.pdf) [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiu6Y2no9PQAhUJS7wKHYB-BjwQFggpMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1506.03340&usg=AFQjCNEhq-W8rar0b08n_Vt3CSkeOFTJLw)]
- Towards Universal Paraphrastic Sentence Embeddings. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjM-u3Jo9PQAhUDbrwKHXETBuAQFggtMAE&url=http%3A%2F%2Fttic.uchicago.edu%2F~wieting%2Fwieting2016ICLR.pdf&usg=AFQjCNFMW6NpxCP9FMXpati4GbmkkCgPWQ)]
- Visualizing and Understanding Neural Models in NLP. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwjszoDFpNPQAhWIxbwKHYlEAV0QFgg2MAI&url=https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fpubs%2Fvisualizing16.pdf&usg=AFQjCNECNXN2Cf42XcZokv2o5sE6RNM41Q)]

### attention and memory

- Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation. [[arxiv](https://arxiv.org/abs/1512.04650)] 
- Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. [[arxiv](https://arxiv.org/abs/1506.07285)]
- <b>A Neural Attention Model for Sentence Summarization</b>. [[pdf](docs/2015/A Neural Attention Model for Sentence Summarization.pdf)] [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiJnNOM-9LQAhVGWrwKHTH3BFAQFggiMAA&url=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FD%2FD15%2FD15-1044.pdf&usg=AFQjCNFw54b4APRJXCmpk2Yf0ttsATDZNA)]
- <b>Effective Approaches to Attention-based Neural Machine Translation</b>. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjhkfGlotjQAhXFULwKHTzhA1YQFggmMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1508.04025&usg=AFQjCNHOjFZaEvp3YSwQTEhrgmmLW6ER8g)]
- End-to-End Attention-based Large Vocabulary Speech Recognition. [[url](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwi7y9Dbvo7RAhVF0FQKHa3SCAoQFggsMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1508.04395&usg=AFQjCNE-iJp1IdJm552na5qzcFJNQ85cxA)]