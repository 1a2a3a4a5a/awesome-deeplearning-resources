## deep reinforcement learning

- [citation:51+] <b>Asynchronous Methods for Deep Reinforcement Learning</b>, V. Mnih et al., *arXiv*. [[url](http://arxiv.org/abs/1602.01783)]
- [citation:17+] Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning, E. Parisotto, et al., *ICLR*. [[url](http://arxiv.org/abs/1511.06342)]
- [citation:?] A New Softmax Operator for Reinforcement Learning.[[url](https://128.84.21.199/abs/1612.05628?context=cs)]
- [citation:19+] Benchmarking Deep Reinforcement Learning for Continuous Control, Y. Duan et al., *ICML*. [[url](https://arxiv.org/abs/1604.06778)]
- [citation:9+] Better Computer Go Player with Neural Network and Long-term Prediction, Y. Tian et al., *ICLR*. [[url](http://arxiv.org/abs/1511.06410)]
- [citation:16+] Deep Reinforcement Learning in Parameterized Action Space, M. Hausknecht et al., *ICLR*. [[url](http://arxiv.org/abs/1511.04143)]
- [citation:2+] Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks, R. Houthooft et al., *arXiv*. [[url](http://arxiv.org/abs/1605.09674)]
- [citation:9+] Control of Memory, Active Perception, and Action in Minecraft, J. Oh et al., *ICML*. [[url](http://arxiv.org/abs/1605.09128)]
- [citation:17+] Continuous Deep Q-Learning with Model-based Acceleration, S. Gu et al., *ICML*, 2016. [[url](http://arxiv.org/abs/1603.00748)]
- [citation:85+] <b>Continuous control with deep reinforcement learning</b>, T. P. Lillicrap et al., *ICLR*. [[url](http://arxiv.org/abs/1509.02971)]
- [citation:2+] Deep Successor Reinforcement Learning, T. D. Kulkarni et al., *arXiv*. [[url](http://arxiv.org/abs/1606.02396)]
- [citation:?] Dynamic Frame skip Deep Q Network, A. S. Lakshminarayanan et al., *IJCAI Deep RL Workshop*. [[url](http://arxiv.org/abs/1605.05365)]
- [citation:18+] <b>Deep Exploration via Bootstrapped DQN</b>, I. Osband et al., *arXiv*. [[url](http://arxiv.org/abs/1602.04621)]
- [citation:16+] <b>Deep Reinforcement Learning in Parameterized Action Space</b>, M. Hausknecht et al., *ICLR*. [[url](http://arxiv.org/abs/1511.04143)]
- [citation:?+] Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments.[[url](https://scirate.com/arxiv/1612.05533)]
- [citation:12+] Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization, C. Finn et al., *arXiv*. [[url](http://arxiv.org/abs/1603.00448)]
- [citation:2+] Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks, R. Krishnamurthy et al., *arXiv*. [[url](https://arxiv.org/abs/1605.05359)]
- [citation:14+] Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation, T. D. Kulkarni et al., *arXiv*. [[url](https://arxiv.org/abs/1604.06057)]
- [citation:21+] High-Dimensional Continuous Control Using Generalized Advantage Estimation, J. Schulman et al., *ICLR*. [[url](http://arxiv.org/abs/1506.02438)]
- [citation:11+] Increasing the Action Gap: New Operators for Reinforcement Learning, M. G. Bellemare et al., *AAAI*. [[url](http://arxiv.org/abs/1512.04860)]
- [citation:27+] Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection, S. Levine et al., *arXiv*. [[url](http://arxiv.org/abs/1603.02199)]
- [citation:6+] Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks, J. N. Foerster et al., *arXiv*. [[url](http://arxiv.org/abs/1602.02672)]
- [citation:?] Loss is its own Reward: Self-Supervision for Reinforcement Learning.[[url](https://arxiv.org/abs/1612.07307)]
- [citation:5+] Model-Free Episodic Control, C. Blundell et al., *arXiv*. [[url](http://arxiv.org/abs/1606.04460)]
- [citation:397+] <b>Mastering the game of Go with deep neural networks and tree search</b>, D. Silver et al., *Nature*. [[url](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)]
- [citation:6+] MazeBase: A Sandbox for Learning from Games, S. Sukhbaatar et al., *arXiv* [[url](http://arxiv.org/abs/1511.07401)]
- [citation:?] Online Sequence-to-Sequence Active Learning for Open-Domain Dialogue Generation. *arXiv*. [[url](https://arxiv.org/abs/1612.03929)]
- [citation:10+] Policy Distillation, A. A. Rusu et at., *ICLR*. [[url](http://arxiv.org/abs/1511.06295)]
- [citation:41+] <b>Prioritized Experience Replay</b>, T. Schaul et al., *ICLR*. [[url](http://arxiv.org/abs/1511.05952)]
- [citation:4+] Safe and Efficient Off-Policy Reinforcement Learning, R. Munos et al., *arXiv*. [[url](https://arxiv.org/abs/1606.02647)]
- [citation:?] Sample-efficient Deep Reinforcement Learning for Dialog Control.*arXiv*.[[url](https://scirate.com/arxiv/1612.06000)]
- [citation:?] Self-Correcting Models for Model-Based Reinforcement Learning.[[url](https://scirate.com/arxiv/1612.06018)]
- [citation:8+] Unifying Count-Based Exploration and Intrinsic Motivation, M. G. Bellemare et al., *arXiv*. [[url](https://arxiv.org/abs/1606.01868)]
- [citation:3+] Value Iteration Networks, A. Tamar et al., *arXiv*. [[url](http://arxiv.org/abs/1602.02867)]
